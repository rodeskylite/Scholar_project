{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 403: {\"error\":\"Invalid query parameters error.\",\"message\":\"citation_count is not a valid field. Valid fields are underscore or hyphenated versions of: abstract.search, abstract.search.no_stem, apc_list.currency, apc_list.provenance, apc_list.value, apc_list.value_usd, apc_paid.currency, apc_paid.provenance, apc_paid.value, apc_paid.value_usd, author.id, author.orcid, authors_count, authorships.affiliations.institution_ids, authorships.author.id, authorships.author.orcid, authorships.countries, authorships.institutions.continent, authorships.institutions.country_code, authorships.institutions.id, authorships.institutions.is_global_south, authorships.institutions.lineage, authorships.institutions.ror, authorships.institutions.type, authorships.is_corresponding, best_oa_location.is_accepted, best_oa_location.is_oa, best_oa_location.is_published, best_oa_location.landing_page_url, best_oa_location.license, best_oa_location.license_id, best_oa_location.source.host_organization, best_oa_location.source.host_organization_lineage, best_oa_location.source.id, best_oa_location.source.is_in_doaj, best_oa_location.source.is_oa, best_oa_location.source.issn, best_oa_location.source.type, best_oa_location.version, best_open_version, biblio.first_page, biblio.issue, biblio.last_page, biblio.volume, citation_normalized_percentile.is_in_top_10_percent, citation_normalized_percentile.is_in_top_1_percent, citation_normalized_percentile.value, cited_by, cited_by_count, cited_by_percentile_year.max, cited_by_percentile_year.min, cites, concept.id, concepts.id, concepts.wikidata, concepts_count, corresponding_author_ids, corresponding_institution_ids, countries_distinct_count, datasets, default.search, display_name, display_name.search, display_name.search.no_stem, doi, doi_starts_with, from_created_date, from_publication_date, fulltext.search, fulltext_origin, fwci, grants.award_id, grants.funder, has_abstract, has_doi, has_embeddings, has_fulltext, has_oa_accepted_or_published_version, has_oa_submitted_version, has_old_authors, has_orcid, has_pdf_url, has_pmcid, has_pmid, has_raw_affiliation_strings, has_references, ids.mag, ids.openalex, ids.pmcid, ids.pmid, indexed_in, institution.id, institution_assertions.country_code, institution_assertions.id, institution_assertions.lineage, institution_assertions.ror, institution_assertions.type, institutions.continent, institutions.country_code, institutions.id, institutions.is_global_south, institutions.ror, institutions.type, institutions_distinct_count, is_corresponding, is_oa, is_paratext, is_retracted, journal, keyword.search, keywords.id, language, locations.is_accepted, locations.is_oa, locations.is_published, locations.landing_page_url, locations.license, locations.license_id, locations.source.has_issn, locations.source.host_institution_lineage, locations.source.host_organization, locations.source.host_organization_lineage, locations.source.id, locations.source.is_core, locations.source.is_in_doaj, locations.source.is_oa, locations.source.issn, locations.source.publisher_lineage, locations.source.type, locations.version, locations_count, mag, mag_only, oa_status, open_access.any_repository_has_fulltext, open_access.is_oa, open_access.oa_status, openalex, openalex_id, pmcid, pmid, primary_location.is_accepted, primary_location.is_oa, primary_location.is_published, primary_location.landing_page_url, primary_location.license, primary_location.license_id, primary_location.source.has_issn, primary_location.source.host_institution_lineage, primary_location.source.host_organization, primary_location.source.host_organization_lineage, primary_location.source.id, primary_location.source.is_core, primary_location.source.is_in_doaj, primary_location.source.is_oa, primary_location.source.issn, primary_location.source.publisher_lineage, primary_location.source.type, primary_location.version, primary_topic.domain.id, primary_topic.field.id, primary_topic.id, primary_topic.subfield.id, publication_date, publication_year, raw_affiliation_strings.search, raw_author_name.search, referenced_works, referenced_works_count, related_to, repository, semantic.search, sustainable_development_goals.id, sustainable_development_goals.score, title.search, title.search.no_stem, title_and_abstract.search, title_and_abstract.search.no_stem, to_created_date, to_publication_date, to_updated_date, topics.domain.id, topics.field.id, topics.id, topics.subfield.id, topics_count, type, type_crossref, version\"}\n",
      "\n",
      "No se encontraron resultados.\n"
     ]
    }
   ],
   "source": [
    "# Configurar la URL base de OpenAlex\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "# FunciÃ³n para buscar papers\n",
    "def search_papers(query, limit=50):\n",
    "    params = {\n",
    "        \"filter\": \"abstract.search:landslides,publication_year:2020-2025\",\n",
    "        \"sort\": \"citation_count:desc\",\n",
    "        \"per_page\": 50\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"results\"]\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Buscar papers sobre el tema \"machine learning and wildfires\"\n",
    "query = \"machine learning and lanslides\"\n",
    "papers = search_papers(query, limit=50)\n",
    "\n",
    "# Procesar los resultados\n",
    "if papers:\n",
    "    data = []\n",
    "    for paper in papers:\n",
    "        data.append({\n",
    "            \"Title\": paper.get(\"title\"),\n",
    "            \"Authors\": \", \".join([author[\"author\"][\"display_name\"] for author in paper.get(\"authorships\", [])]),\n",
    "            \"Year\": paper.get(\"publication_year\"),\n",
    "            \"DOI\": paper.get(\"doi\"),\n",
    "            \"URL\": paper.get(\"id\"),\n",
    "            \"Abstract\": paper.get(\"abstract_inverted_index\"),\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame y guardar los datos en un archivo CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"openalex_papers.csv\", index=False)\n",
    "    print(\"Base de datos generada exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     39\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers:\n\u001b[0;32m     41\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthors\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([author[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthorships\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m author\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m)]),\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublication_year\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mreconstruct_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract_inverted_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     })\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Crear un DataFrame y guardar los datos en un archivo CSV\u001b[39;00m\n\u001b[0;32m     51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mreconstruct_abstract\u001b[1;34m(inverted_index)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, positions \u001b[38;5;129;01min\u001b[39;00m inverted_index\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m positions:\n\u001b[1;32m---> 14\u001b[0m         \u001b[43mabstract\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m word\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(abstract)\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar la URL base de OpenAlex\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "# FunciÃ³n para reconstruir abstracts desde el Ã­ndice invertido\n",
    "def reconstruct_abstract(inverted_index):\n",
    "    if not inverted_index:\n",
    "        return \"\"\n",
    "    abstract = [\"\"] * max([max(pos_list) for pos_list in inverted_index.values()])\n",
    "    for word, positions in inverted_index.items():\n",
    "        for pos in positions:\n",
    "            abstract[pos] = word\n",
    "    return \" \".join(abstract)\n",
    "\n",
    "# FunciÃ³n para buscar papers\n",
    "def search_papers(query, limit=50):\n",
    "    params = {\n",
    "        \"filter\": f\"abstract.search:{query},publication_year:2020-2025\",\n",
    "        \"sort\": \"cited_by_count:desc\",  # Cambiado \"citation_count\" por \"cited_by_count\"\n",
    "        \"per_page\": limit\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"results\", [])\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Buscar papers sobre \"machine learning and landslides\"\n",
    "query = \"machine learning and landslides\"\n",
    "papers = search_papers(query, limit=50)\n",
    "\n",
    "# Procesar los resultados\n",
    "if papers:\n",
    "    data = []\n",
    "    for paper in papers:\n",
    "        data.append({\n",
    "            \"Title\": paper.get(\"title\"),\n",
    "            \"Authors\": \", \".join([author[\"author\"][\"display_name\"] for author in paper.get(\"authorships\", []) if author.get(\"author\")]),\n",
    "            \"Year\": paper.get(\"publication_year\"),\n",
    "            \"DOI\": paper.get(\"doi\"),\n",
    "            \"URL\": paper.get(\"id\"),\n",
    "            \"Abstract\": reconstruct_abstract(paper.get(\"abstract_inverted_index\"))\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame y guardar los datos en un archivo CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"openalex_papers.csv\", index=False)\n",
    "    print(\"Base de datos generada exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos generada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar la URL base de OpenAlex\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "# FunciÃ³n para reconstruir abstracts desde el Ã­ndice invertido\n",
    "def reconstruct_abstract(inverted_index):\n",
    "    if not inverted_index:\n",
    "        return \"\"  # Si el abstract estÃ¡ vacÃ­o, devolver una cadena vacÃ­a\n",
    "    \n",
    "    positions = []\n",
    "    for word, pos_list in inverted_index.items():\n",
    "        positions.extend(pos_list)\n",
    "    \n",
    "    if not positions:\n",
    "        return \"\"  # Si no hay posiciones vÃ¡lidas, devolver una cadena vacÃ­a\n",
    "\n",
    "    max_pos = max(positions)\n",
    "    \n",
    "    # Si max_pos es menor que 0, significa que no hay valores vÃ¡lidos\n",
    "    if max_pos < 0:\n",
    "        return \"\"\n",
    "\n",
    "    abstract = [\"\"] * (max_pos + 1)  # Crear una lista con el tamaÃ±o correcto\n",
    "    \n",
    "    for word, pos_list in inverted_index.items():\n",
    "        for pos in pos_list:\n",
    "            if 0 <= pos < len(abstract):  # Evitar IndexError\n",
    "                abstract[pos] = word\n",
    "\n",
    "    return \" \".join(abstract).strip()\n",
    "\n",
    "# FunciÃ³n para buscar papers\n",
    "def search_papers(query, limit=50):\n",
    "    params = {\n",
    "        \"filter\": f\"abstract.search:{query},publication_year:2020-2025\",\n",
    "        \"sort\": \"cited_by_count:desc\",  # Cambiado \"citation_count\" por \"cited_by_count\"\n",
    "        \"per_page\": limit\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"results\", [])\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Buscar papers sobre \"machine learning and landslides\"\n",
    "query = \"machine learning and landslides\"\n",
    "papers = search_papers(query, limit=50)\n",
    "\n",
    "# Procesar los resultados\n",
    "if papers:\n",
    "    data = []\n",
    "    for paper in papers:\n",
    "        data.append({\n",
    "            \"Title\": paper.get(\"title\"),\n",
    "            \"Authors\": \", \".join([author[\"author\"][\"display_name\"] for author in paper.get(\"authorships\", []) if author.get(\"author\")]),\n",
    "            \"Year\": paper.get(\"publication_year\"),\n",
    "            \"DOI\": paper.get(\"doi\"),\n",
    "            \"URL\": paper.get(\"id\"),\n",
    "            \"Abstract\": reconstruct_abstract(paper.get(\"abstract_inverted_index\"))\n",
    "        })\n",
    "\n",
    "    # Crear un DataFrame y guardar los datos en un archivo CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"openalex_papers.csv\", index=False)\n",
    "    print(\"Base de datos generada exitosamente.\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scholar_api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
